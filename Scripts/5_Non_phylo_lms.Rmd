---
title: "Non-phylogenetic Analysis and Plots"
output: html_notebook
---

# Introduction

This notebook aims to fit ordinary linear regression models in order to determine the nature of the relationships between the 10 continuous traits WITHOUT assuming any phylogenetic affect. This can then be compared to the similar analysis using pgls to determine the effect of phylogeny on the results calculated. Since this is a simple linear regression, the parameters should be the same when the axes are reversed and hence only half the number of pairwise correlations need to be carried out for determining p-value and r-squared, though that is not true for the slope coefficient so all axis arrangements were run anyway.

The dataset has been pruned to only include the 160 taxa used in the pgls analysis for comparability. Note: this means the tree is required input for this script so make sure the newick file for that has been created previously (and is in the input location) before attempting to run this code.

This notebook also aims to create network diagrams and an ellipse table to represent the results and compare to the phylogenetic analysis. Also, perhaps even determine how to tweak the graphs such that similar code can be used to plot the phylogenetic analysis results.

# Analysis

The data being used are already log-transformed and scaled from the previous analysis (pgls does not require the data to be altered before use so the same dataset of trait values ca be used for ordinary linear regressions and least squares calculations).

## Trait-Trait Correlations

### Max. Height

```{r}
olm.mh.rh <- lm(max_height_m ~ relative_height_by_girth, data = eucs.scaled.data)
summary(olm.mh.rh)
#summary(lm(relative_height_by_girth ~max_height_m, data= eucs.scaled.data))
summary(max.height.rh) # noticeable decrease in measures of relationship, notable but not dramatic

olm.rh.mh <- lm(relative_height_by_girth ~ max_height_m, data = eucs.scaled.data)
summary(olm.rh.mh)
```
```{r}
olm.mh.sd <- lm(max_height_m ~ stem_density_g_per_ml, data = eucs.scaled.data)
summary(olm.mh.sd)
#summary(lm(stem_density_g_per_ml ~ max_height_m, data = eucs.scaled.data))
summary(max.height.std) # general decrease in all measures of relationship to the end of it no longer being significant

olm.std.mh <- lm(stem_density_g_per_ml ~ max_height_m, data = eucs.scaled.data)
summary(olm.std.mh)
```
```{r}
olm.mh.bark <- lm(max_height_m ~ relative_bt_by_girth, data = eucs.scaled.data)
summary(olm.mh.bark)
summary(max.height.bark) # general notable desrease in all measure of relationship

olm.bark.mh <- lm(relative_bt_by_girth ~ max_height_m, data = eucs.scaled.data)
summary(olm.bark.mh)
```

```{r}
olm.mh.sla <- lm(max_height_m ~ sla_mm2_per_mg, data = eucs.scaled.data)
summary(olm.mh.sla)
summary(max.height.sla) # general notable decrease in relationship

olm.sla.mh <- lm(sla_mm2_per_mg ~ max_height_m, data = eucs.scaled.data)
summary(olm.sla.mh)
```

```{r}
olm.mh.lfa <- lm(max_height_m ~ leaf_area_cm2, data = eucs.scaled.data)
summary(olm.mh.lfa)
summary(max.height.lfa) # general decrease in relationship

olm.lfa.mh <- lm(leaf_area_cm2 ~ max_height_m, data = eucs.scaled.data)
summary(olm.lfa.mh)
```

```{r}
olm.mh.lms <- lm(max_height_m ~ leaf_mass_g, data = eucs.scaled.data)
summary(olm.mh.lms)
summary(max.height.lms) # intro of L ~ 0.8 decreases the strength and certainty of the relationship
#summary(leaf.mass.mh)

olm.lms.mh <- lm(leaf_mass_g ~ max_height_m, data = eucs.scaled.data)
summary(olm.lms.mh)
```
```{r}
olm.mh.fwl <- lm(max_height_m ~ fruit_wall_width_mm, data = eucs.scaled.data)
summary(olm.mh.fwl)
summary(max.height.fwl) # L ~ 0.775 destroys most evidence for a measureable relationship

olm.fwl.mh <- lm(fruit_wall_width_mm ~ max_height_m, data = eucs.scaled.data)
summary(olm.fwl.mh)
```

```{r}
olm.mh.fms <- lm(max_height_m ~ fruit_mass_mg, data = eucs.scaled.data)
summary(olm.mh.fms)
summary(max.height.fms) # intro of L ~ 0.8 causes borderline-certain relationship to become incredibly unlikely and the relationship is destroyed

olm.fms.mh <- lm(fruit_mass_mg ~ max_height_m, data = eucs.scaled.data)
summary(olm.fms.mh)
```

```{r}
olm.mh.sms <- lm(max_height_m ~ seed_mass_mg, data = eucs.scaled.data)
#summary(lm(seed_mass_mg ~ max_height_m, data = eucs.scaled.data))
summary(max.height.sms) #becomes unlikely that there's a relationship at all once L ~ 0.8

olm.sms.mh <- lm(seed_mass_mg ~ max_height_m, data = eucs.scaled.data)
summary(olm.sms.mh)
```

### Relative Height

```{r}
olm.rh.std <- lm(relative_height_by_girth ~ stem_density_g_per_ml, data = eucs.scaled.data)
summary(olm.rh.std)
#summary(olm.std.rh)#these two models have very similar slopes though identical r-squared and p-values
summary(rel.height.std) #near identical but slightly more certain after L~0.4

olm.std.rh <- lm(stem_density_g_per_ml ~ relative_height_by_girth, data = eucs.scaled.data)
summary(olm.std.rh)
summary(stem.density.rh) #L~0.7 causes slightly more of an increase in certainty but not much
```
RH and SD no longer significant relations now in either arrangement.

```{r}
olm.rh.bark <- lm(relative_height_by_girth ~ relative_bt_by_girth, data = eucs.scaled.data)
summary(olm.rh.bark)
summary(rel.height.bark) #almost identical

olm.bark.rh <- lm(relative_bt_by_girth ~ relative_height_by_girth,data = eucs.scaled.data)
summary(olm.bark.rh)
summary(bark.rh) #intro of L~0.7 destroys the relationship entirely
```

```{r}
olm.rh.sla <- lm(relative_height_by_girth ~ sla_mm2_per_mg, data = eucs.scaled.data)
summary(olm.rh.sla)
summary(rel.height.sla) #notable increase in certainty despite L=0

olm.sla.rh <- lm(sla_mm2_per_mg ~ relative_height_by_girth, data = eucs.scaled.data)
summary(olm.sla.rh)
summary(sla.rh) # intro of L~0.8 increased uncertainty but still significant
```

```{r}
olm.rh.lfa <- lm(relative_height_by_girth ~ leaf_area_cm2, data = eucs.scaled.data)
summary(olm.rh.lfa)
summary(rel.height.lfa) #near identical

olm.lfa.rh <- lm(leaf_area_cm2 ~ relative_height_by_girth, data = eucs.scaled.data)
summary(olm.lfa.rh)
summary(leaf.area.rh) #intro of L~0.5 notably increased uncertainty but still significant
```

```{r}
olm.rh.lms <- lm(relative_height_by_girth ~ leaf_mass_g, data = eucs.scaled.data)
summary(olm.rh.lms)
summary(rel.height.lms) #very similar

olm.lms.rh <- lm(leaf_mass_g ~ relative_height_by_girth, data = eucs.scaled.data)
summary(olm.lms.rh)
summary(leaf.mass.rh) #still very similar bc L still 0
```

```{r}
olm.rh.fwl <- lm(relative_height_by_girth ~ fruit_wall_width_mm, data = eucs.scaled.data)
summary(olm.rh.fwl)
summary(rel.height.fwl) #slightly more certain but very similar since lambda is still zero (hence we don't this is as relaible as the swapped-axes version of this)

olm.fwl.rh <- lm(fruit_wall_width_mm ~ relative_height_by_girth, data = eucs.scaled.data)
summary(olm.fwl.rh)
summary(fruit.wall.rh)# L ~ 0.6 completely destroys any sense of a relationship and uncertainty almost entire
```

```{r}
olm.rh.fms <- lm(relative_height_by_girth ~ fruit_mass_mg, data = eucs.scaled.data) #almost identical to lambda-0 pgls
summary(olm.rh.fms)
summary(rel.height.fms)

olm.fms.rh <- lm(fruit_mass_mg ~ relative_height_by_girth, data = eucs.scaled.data)
summary(olm.fms.rh)
summary(fruit.mass.rh) #slightly less correlated than OLS despite L~0.7
```

```{r}
olm.rh.sms <- lm(relative_height_by_girth ~ seed_mass_mg, data = eucs.scaled.data)
summary(olm.rh.sms)
summary(rel.height.sms) #notably less correlated despite have L=0

olm.sms.rh <- lm(seed_mass_mg ~ relative_height_by_girth, data = eucs.scaled.data)
summary(olm.sms.rh)
summary(seed.mass.rh) #actually increased in certainty! but uncertainty still too great
```

### Stem Density

```{r}
olm.std.bark <- lm(stem_density_g_per_ml ~ relative_bt_by_girth, data = eucs.scaled.data)
summary(olm.std.bark)
summary(stem.density.bark) # L ~ 0.65 dramatically increases uncertainty, though still significant

olm.bark.std <- lm(relative_bt_by_girth ~ stem_density_g_per_ml, data = eucs.scaled.data)
summary(olm.bark.std)
```

```{r}
olm.std.sla <- lm(stem_density_g_per_ml ~ sla_mm2_per_mg, data = eucs.scaled.data)
summary(olm.std.sla)
summary(stem.density.sla) # intro of L ~ 0.4 slightly increases uncertainty but still very much significant

olm.sla.std <- lm(sla_mm2_per_mg ~ stem_density_g_per_ml, data = eucs.scaled.data)
summary(olm.sla.std)
```

```{r}
olm.std.lfa <- lm(stem_density_g_per_ml ~ leaf_area_cm2, data = eucs.scaled.data)
summary(olm.std.lfa)
summary(stem.density.lfa) #introof L ~ 0.5 increases uncertainty dramatically and relationship is no longer significant

olm.lfa.std <- lm(leaf_area_cm2 ~ stem_density_g_per_ml, data = eucs.scaled.data)
summary(olm.lfa.std)
```


```{r}
olm.std.lms <- lm(stem_density_g_per_ml ~ leaf_mass_g, data = eucs.scaled.data)
summary(olm.std.lms)
summary(stem.density.lms) #intro of L ~ 0.56 dramatically decreases uncertainty, though still non-significant

olm.lms.std <- lm(leaf_mass_g ~ stem_density_g_per_ml, data = eucs.scaled.data)
summary(olm.lms.std)
```
```{r}
olm.std.fwl <- lm(stem_density_g_per_ml ~ fruit_wall_width_mm, data = eucs.scaled.data)
summary(olm.std.fwl)
summary(stem.density.fwl) # L ~ 0.56, yet barely any change beyond slight decrease in uncertainty

olm.fwl.std <- lm(fruit_wall_width_mm ~ stem_density_g_per_ml, data = eucs.scaled.data)
summary(olm.fwl.std)
```

```{r}
olm.std.fms <- lm(stem_density_g_per_ml ~ fruit_mass_mg, data = eucs.scaled.data)
summary(olm.std.fms)
summary(stem.density.fms) #slight decrease in uncertainty from L ~ 0.56

olm.fms.std <- lm(fruit_mass_mg ~ stem_density_g_per_ml, data = eucs.scaled.data)
summary(olm.fms.std)
```
```{r}
olm.std.sms <- lm(stem_density_g_per_ml ~ seed_mass_mg, data = eucs.scaled.data)
summary(olm.std.sms)
summary(stem.density.sms) # very similar though the direction of the relationship did change a bit, however the relationship is very non significant that that doesn't matter much

olm.sms.std <- lm(seed_mass_mg ~ stem_density_g_per_ml, data = eucs.scaled.data)
summary(olm.sms.std)
```



### Relative Bark Thickness

```{r}
olm.bark.sla <- lm(relative_bt_by_girth ~ sla_mm2_per_mg, data = eucs.scaled.data)
summary(olm.bark.sla)
summary(bark.sla)# intro of L ~ 0.7 increases uncertainty a lot but still significant

olm.sla.bark <- lm(sla_mm2_per_mg ~ relative_bt_by_girth, data = eucs.scaled.data)
summary(olm.sla.bark)
```

```{r}
olm.bark.lfa <- lm(relative_bt_by_girth ~ leaf_area_cm2, data = eucs.scaled.data)
summary(olm.bark.lfa)
summary(bark.lfa) #L ~ 0.6 increased uncertainty but still significant

olm.lfa.bark <- lm(leaf_area_cm2 ~ relative_bt_by_girth, data = eucs.scaled.data)
summary(olm.lfa.bark)
```

```{r}
olm.bark.lms <- lm(relative_bt_by_girth ~ leaf_mass_g, data = eucs.scaled.data)
summary(olm.bark.lms)
summary(bark.lms) #L ~ 0.7 destroys relationship and is no longer signifcant at all

olm.lms.bark <- lm(leaf_mass_g ~ relative_bt_by_girth, data = eucs.scaled.data)
summary(olm.lms.bark)
```


```{r}
olm.bark.fwl <- lm(relative_bt_by_girth ~ fruit_wall_width_mm, data = eucs.scaled.data)
summary(olm.bark.fwl)
summary(bark.fwl) # introof L ~ 0.7 increases uncertainty of relationship but still manages to be significant

olm.fwl.bark <- lm(fruit_wall_width_mm ~ relative_bt_by_girth, data = eucs.scaled.data)
summary(olm.fwl.bark)
```


```{r}
olm.bark.fms <- lm(relative_bt_by_girth ~ fruit_mass_mg, data = eucs.scaled.data)
summary(olm.bark.fms)
summary(bark.fms) # L ~ 0.68 somewhat increases uncertainty but otherwise very little change

olm.fms.bark <- lm(fruit_mass_mg ~ relative_bt_by_girth, data = eucs.scaled.data)
summary(olm.fms.bark)
```


```{r}
olm.bark.sms <- lm(relative_bt_by_girth ~ seed_mass_mg, data = eucs.scaled.data)
summary(olm.bark.sms)
summary(bark.sms) # somewhat increases uncertainty and becomes non-significant since it was only ever pretty borderline anyway

olm.sms.bark <- lm(seed_mass_mg ~ relative_bt_by_girth, data = eucs.scaled.data)
summary(olm.sms.bark)
```


### SLA

```{r}
olm.sla.lfa <- lm(sla_mm2_per_mg ~ leaf_area_cm2, data = eucs.scaled.data)
summary(olm.sla.lfa)
summary(sla.lfa) # L ~ 0.8 completely destroys the relationship

olm.lfa.sla <- lm(leaf_area_cm2 ~ sla_mm2_per_mg, data = eucs.scaled.data)
summary(olm.lfa.sla)
```


```{r}
olm.sla.lms <- lm(sla_mm2_per_mg ~ leaf_mass_g, data = eucs.scaled.data)
summary(olm.sla.lms)
summary(sla.lms) # L ~ 0.8 DECREASED the uncertainty to be very significant

olm.lms.sla <- lm(leaf_mass_g ~ sla_mm2_per_mg, data = eucs.scaled.data)
summary(olm.lms.sla)
```


```{r}
olm.sla.fwl <- lm(sla_mm2_per_mg ~ fruit_wall_width_mm, data = eucs.scaled.data)
summary(olm.sla.fwl)
summary(sla.fwl) #slight increase in uncertainty after L ~ 0.65 incorporated

olm.fwl.sla <- lm(fruit_wall_width_mm ~ sla_mm2_per_mg, data = eucs.scaled.data)
summary(olm.fwl.sla)
```

```{r}
olm.sla.fms <- lm(sla_mm2_per_mg ~ fruit_mass_mg, data = eucs.scaled.data)
summary(olm.sla.fms)
summary(sla.fms) #L ~ 0.7 slughtly DECREASES uncertainty and increases effect size

olm.fms.sla <- lm(fruit_mass_mg ~ sla_mm2_per_mg, data = eucs.scaled.data)
summary(olm.fms.sla)
```

```{r}
olm.sla.sms <- lm(sla_mm2_per_mg ~ seed_mass_mg, data = eucs.scaled.data)
summary(olm.sla.sms)
summary(sla.sms) # L ~ 0.76 somwhat DECREASED uncertainty and increasedeffect size

olm.sms.sla <- lm(seed_mass_mg ~ sla_mm2_per_mg, data = eucs.scaled.data)
summary(olm.sms.sla)
```

### Leaf Area

```{r}
olm.lfa.lms <- lm(leaf_area_cm2 ~ leaf_mass_g, data = eucs.scaled.data)
summary(olm.lfa.lms)
summary(leaf.area.lms) #intro of L ~ 0.8 does not change uncertainty but slightly decreases effect size, though slightly increases R-squared
#FIRST TIME decrease in effect size has not been accompanied by decrease in r-squared

olm.lms.lfa <- lm(leaf_mass_g ~ leaf_area_cm2, data = eucs.scaled.data)
summary(olm.lms.lfa)
```

```{r}
olm.lfa.fwl <- lm(leaf_area_cm2 ~ fruit_wall_width_mm, data = eucs.scaled.data)
summary(olm.lfa.fwl)
summary(leaf.area.fwl) #intro of L ~ 0.7 dramatically increases effect size and decreases uncertainty, causing the relationship to become significant

olm.fwl.lfa <- lm(fruit_wall_width_mm ~ leaf_area_cm2, data = eucs.scaled.data)
summary(olm.fwl.lfa)
```
```{r}
olm.lfa.fms <- lm(leaf_area_cm2 ~ fruit_mass_mg, data = eucs.scaled.data)
summary(olm.lfa.fms)
summary(leaf.area.fms) # L ~ 0.66 causes DECREASE in uncertainty as well as increase in effect size and proportion of accounted-for variation

olm.fms.lfa <- lm(fruit_mass_mg ~ leaf_area_cm2, data = eucs.scaled.data)
summary(olm.fms.lfa)
```

```{r}
olm.lfa.sms <- lm(leaf_area_cm2 ~ seed_mass_mg, data = eucs.scaled.data)
summary(olm.lfa.sms)
summary(leaf.area.sms) # L ~ 0.65 increases effect size and r-squred and dramatically decreasing the uncertainty along, causing the relationship to become significantly observable

olm.sms.lfa <- lm(seed_mass_mg ~ leaf_area_cm2, data = eucs.scaled.data)
summary(olm.sms.lfa)
```


### Leaf Mass

```{r}
olm.lms.fwl <- lm(leaf_mass_g ~ fruit_wall_width_mm, data = eucs.scaled.data)
summary(olm.lms.fwl)
summary(leaf.mass.fwl) # intro of L ~ 0.6 sees increase in strength of relationship, effect size and decrease in uncertainty, though it was already significant anyway

olm.fwl.lms <- lm(fruit_wall_width_mm ~ leaf_mass_g, data = eucs.scaled.data)
summary(olm.fwl.lms)
```

```{r}
olm.lms.fms <- lm(leaf_mass_g ~ fruit_mass_mg, data = eucs.scaled.data)
summary(olm.lms.fms)
summary(leaf.mass.fms) #L ~ 0.6 significantly increases the strength of the relationship

olm.fms.lms <- lm(fruit_mass_mg ~ leaf_mass_g, data = eucs.scaled.data)
summary(olm.fms.lms)
```


```{r}
olm.lms.sms <- lm(leaf_mass_g ~ seed_mass_mg, data = eucs.scaled.data)
summary(olm.lms.sms)
summary(leaf.mass.sms) # L~ 0.6 significantly increases the strength of the relationship and decreases the uncertainty

olm.sms.lms <- lm(seed_mass_mg ~ leaf_mass_g, data = eucs.scaled.data)
summary(olm.sms.lms)
```


### Fruit Wall Width

```{r}
olm.fwl.fms <- lm(fruit_wall_width_mm ~ fruit_mass_mg, data = eucs.scaled.data)
summary(olm.fwl.fms)
summary(fruit.wall.fms) # L ~ 0.6 though the results are very much similar

olm.fms.fwl <- lm(fruit_mass_mg ~ fruit_wall_width_mm, data = eucs.scaled.data)
summary(olm.fms.fwl)
```

```{r}
olm.fwl.sms <- lm(fruit_wall_width_mm ~ seed_mass_mg, data = eucs.scaled.data)
summary(olm.fwl.sms)
summary(fruit.wall.sms) # the effect size increases but nothing else really changes

olm.sms.fwl <- lm(seed_mass_mg ~ fruit_wall_width_mm, data = eucs.scaled.data)
summary(olm.sms.fwl)
```


### Fruit Mass

```{r}
olm.fms.sms <- lm(fruit_mass_mg ~ seed_mass_mg, data = eucs.scaled.data)
summary(olm.fms.sms)
summary(fruit.mass.sms) # L ~ 0.5 increases effect size but decreases r-squared

olm.sms.fms <- lm(seed_mass_mg ~ fruit_mass_mg,data = eucs.scaled.data)
summary(olm.sms.fms)
```


So when does accounting for phylogeny decrease the effect size/certainty/proportion of variation accounted for?
  - Mh-stem and leaf traits decrease notably
      - mh-sd no longer significant
      - Mh-fww goes from significant to non-significant, though the actual change in measures of the strength of the relationship aren't that great
      -mh-lms becomes non-significant now
  - Mh-fm and sm, go from almost certain correlations to almost definitely not
  - all but sms and sd -w rh
  - all but lms, fww and fms with stem density
    -sd-lfa no longer significant
  - all bark correlations
    -bark-lms no longer significant
  - all sla- except lms and rep.traits
      -sla-lfa no longer significant

Those where accounting for phylogeny increased relationship strength:
  -sms-rh
  -sd-rh
  -sd-lms
  -sd-fms
  -sd-fww
  -sla-lms
  -sla-reproductive traits
  -lfa-reproductive traits
      -lfa-seed is now significant
      -lfa fruit wall now significant
  -lms-reproductive traits
  

Weird effects:
  -leaf area-lms increases in effect size but decreases in r-squared
  -fruit wall width-fruit mass slightly increases but remains almost identical
  -fww-sms increased effect size only
  -fms-sms inreased effect size but decreased r-squred
  
  
In general, OLS regression overestimates strength of relationships among habit and stem traits and underestimates them among the reproductive traits. Mostly th effects were not great enough to change significance but there were a few cases where this changed:
  -max.height-sd no longer clearly significant
  -max.height-fruit wall width (fms and sms also no longer borderline)
  -max.height-leaf mass also no longer significant (using upper lambda model)
  -rbt-rh also now not significant (when using the upper lambda otherwise its ambiguous)
  -rbt-lms no longer significant (when using upper lambda)
  -stem.density-leaf area no longer significant
  -sla-lfa no longer significant
  
  -leaf area-seed mass now significant
  -leaf area-fruit wall width now significant
  
This is all for the orientation of the axes that were calculated for the original olms but just need to check any changes in significance that may be different depending on which way around the pgls was since that does occasionally change the significance.

lms-mh
rbt-rh (in the upper lambda)
sd-mh (already accounted for above as the way arounf corresponding to upper half lambda)
lms-bark

Hence rbt-rh is another relationship not previously acconted for and included within the upper lambda half we're going with that resulted in a change in significance, the rest already calculated above will be the same in both directions or will have chosen the upper lambda half arrangement of varibales anyway.


# Plots and Comparisons

## Ellipse-Table

```{r}
table.matrix <- cor(na.omit(eucs.scaled.data[,!names(eucs.scaled.data) %in% c("taxon", "dataset", "dataset2", "Sprouter", "Stem_Sprouter", "Lignotuber_Sprouter", "Stem_Sprouter2", "Lignotuber_Only", "Sprouting_Type", "Sprouting_Type2", "Sieberi")]))
table.matrix

colnames(table.matrix) <- c("MH", "RH", "SD", "RBT", "SLA", "LA", "LM", "FWW", "FM", "SM")
rownames(table.matrix) <- colnames(table.matrix)
corrplot(table.matrix, method = "ellipse")
corrplot(matrix.r, method = 'ellipse')
```
My funciton to try and square the terms while keeping the sign has resulted in some strange sign changes so need to be fixed. The original table just using the correlation coefficients

You can square all the entries in a matrix by squaring the matrix object, since actual matrix operation such that would cause a squared matrix to be multiplied by itself are written differently to standard scalar opertaions that work on each entry separately.

It's possible to generate a correlation matrix from pgls models by extracting the (pseudo-)r-squared values from the pgls models (hopefully I can find a for loop or function for that so I dont have to do it manually) and then taking the square root (and making sure to re-negative the negative correlations) to arrive at the correlation coefficient r.


## Network Diagram


```{r}
qgraph(table.matrix, layout = 'spring', label.cex = 0.9, label.scale = FALSE) # can also get the same network diagram when you calculate the correlation coefficient directly within this function from the raw data by substituting the previously calculated 'table.matrix' with 'cor(na.omit(eucs.scaled.data[,!names(eucs.scaled.data) %in% c("taxon", "location", "second_location", "Sprouter", "Stem_Sprouter", "Lignotuber_Sprouter", "Stem_Sprouter2", "Lignotuber_Only", "Sprouting_Type", "Sieberi")])'
#however if you calculate the coefficients directly the resulting matrix hasn't got row and column names so the node names in the diagram do not have clear designations
```


Now to compare with the corresponding diagram using the pgls model data.

```{r}
qgraph(table.matrix, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
#qgraph(cor(table.matrix, use = 'pairwise'), layout = 'spring', label.cex = 0.9, label.scale = FALSE) #representes the correlation of the correlation

qgraph(matrix.r, layout = 'spring', label.cex = 0.9, label.scale = FALSE) #this is readable with some thought but 

qgraph(matrix.r, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
qgraph(table.matrix, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
```

#### Matrix of Models

Here we create a dataframe of the models so that we can input this into extration-function for the r2, slope, SE and P-values later. Be aware the way it is constructed means it need to be transposed in order to represent the independent and dependent variables on the correct axes, currently it has dependent variable as colnames rather than row names.

```{r}
olm.mh.list <- list(perfect.fit, olm.mh.rh, olm.mh.sd, olm.mh.bark, olm.mh.sla, olm.mh.lfa, olm.mh.lms, olm.mh.fwl, olm.mh.fms, olm.mh.sms)

olm.rh.list <- list(olm.rh.mh, perfect.fit, olm.rh.std, olm.rh.bark, olm.rh.sla, olm.rh.lfa, olm.rh.lms, olm.rh.fwl, olm.rh.fms, olm.rh.sms)

olm.std.list <- list(olm.std.mh, olm.std.rh, perfect.fit, olm.std.bark, olm.std.sla, olm.std.lfa, olm.std.lms, olm.std.fwl, olm.std.fms, olm.std.sms)
  
olm.bark.list <- list(olm.bark.mh, olm.bark.rh, olm.bark.std, perfect.fit, olm.bark.sla, olm.bark.lfa, olm.bark.lms, olm.bark.fwl, olm.bark.fms, olm.bark.sms)

olm.sla.list <- list(olm.sla.mh, olm.sla.rh, olm.sla.std, olm.sla.bark, perfect.fit, olm.sla.lfa, olm.sla.lms, olm.sla.fwl, olm.sla.fms, olm.sla.sms)

olm.lfa.list <- list(olm.lfa.mh, olm.lfa.rh, olm.lfa.std, olm.lfa.bark, olm.lfa.sla, perfect.fit, olm.lfa.lms, olm.lfa.fwl, olm.lfa.fms, olm.lfa.sms)

olm.lms.list <- list(olm.lms.mh, olm.lms.rh, olm.lms.std, olm.lms.bark, olm.lms.sla, olm.lms.lfa, perfect.fit, olm.lms.fwl, olm.lms.fms, olm.lms.sms)

olm.fwl.list <- list(olm.fwl.mh, olm.fwl.rh, olm.fwl.std, olm.fwl.bark, olm.fwl.sla, olm.fwl.lfa, olm.fwl.lms, perfect.fit, olm.fwl.fms, olm.fwl.sms)

olm.fms.list <- list(olm.fms.mh, olm.fms.rh, olm.fms.std, olm.fms.bark, olm.fms.sla, olm.fms.lfa, olm.fms.lms, olm.fms.fwl, perfect.fit, olm.fms.sms)

olm.sms.list <- list(olm.sms.mh, olm.sms.rh, olm.sms.std, olm.sms.bark, olm.sms.sla, olm.sms.lfa, olm.sms.lms, olm.sms.fwl, olm.sms.fms, perfect.fit)

olm.traits.list <- list(olm.mh.list, olm.rh.list, olm.std.list, olm.bark.list, olm.sla.list, olm.lfa.list, olm.lms.list, olm.fwl.list, olm.fms.list, olm.sms.list) #remember this still has the column and titles corresponding to dep_var
```


### Significant lines only

Since the olm r-values are already equal for the upper and lower triangles of the matrix and as such it is already symmetrical and only one value exists for each pairwise comparison, we can skip the part on the pgls netowrk diagrams that makes a symmetrical version of the netowrk diagram. 

However, in order to make this diagram similarly easier to read with fewer lines and comparable to the sig-only pgls network, we need to shoe only those lines that correpsond to significant correlations.

```{r}
#to extact the p-vales from each model systematically
olm.P <- lapply(olm.traits.list, function(model.list){
  unlist(lapply(model.list, extract_P))
})

olm.P <- as.data.frame(olm.P)
colnames(olm.P) <- colnames(table.matrix)
row.names(olm.P) <- colnames(olm.P)
olm.P <- t(olm.P) #now it's around the right way with the row indicating the dependent variable and colun the independent variable
olm.P == t(olm.P) #meant to be symmetrical since the r2 and P don't change when indep and dep variables get swapped, but if using the regression parameters instead of simply the Pearson's correlation coefficient
all.equal(olm.P, t(olm.P)) #TRUE so the differences are only computational

#need to figure out if the pattern of significance has changed or not
sig.models.olm <- olm.P < 0.05
#olm.sig.models.P == sig.models.olm #no changes in significance so the current lines shown in the network diagram are correct.

# #to create a matrix indicating which models in the matrix correspond to significant relations, it is possible to wrote a function that calculates and extracts the p-values based on the parameters given in the models, however, this did not appear possible for the pgls models and so for consistency we will use the same method as previously and construct the significance-matrix manually
# 
# #now to do the same for the non-pgls analysis, though I don't expect it to change much
# sig.models.olm <- data.frame('MH' = c(0,0,0,0,0,0,0,0,0,0), 'RH' = c(1,0,0,0,0,0,0,0,0,0), 'SD' = c(1,0,0,0,0,0,0,0,0,0), 'RBT' = c(1,1,1,0,0,0,0,0,0,0), 'SLA' = c(1,1,1,1,0,0,0,0,0,0), 'LA' = c(1,1,1,1,1,0,0,0,0,0), 'LM' = c(1,1,0,1,1,1,0,0,0,0), 'FWW' = c(1,0,0,1,1,0,1,0,0,0), 'FM' = c(0,0,0,1,1,1,1,1,0,0), 'SM' = c(0,0,0,1,1,0,1,1,1,0)) #will need to add this upper-triangle to its transpose to get the full one
# 
# #now that the grampiand SD data are excluded, the RH ~ SD relationships is no longer significant sothat's the version we'll be using here
# 
# dim(sig.models.olm) #all good
# as.matrix(sig.models.olm)
#   t(sig.models.olm)
# sig.models.olm <- sig.models.olm + t(sig.models.olm)
# sig.models.olm <- sig.models.olm == 1


sig.only.r.olm <- table.matrix
sig.only.r.olm[!sig.models.olm] <- 0
```


Now the network diagram with only significant lines:


Compared with the full version of the olm network containing all associations:

```{r}
qgraph(sig.only.r.olm, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
qgraph(table.matrix, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
```


Compared with the pgls sig-only network:

```{r}
qgraph(sig.only.r.olm, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
qgraph(sig.only.r.half, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
```


Now to save/export the figures:

```{r}
network.diag.olm.sig <- qgraph(sig.only.r.olm, layout = 'spring', label.cex = 0.9, label.scale = FALSE)

qgraph(network.diag.olm.sig, filetype = 'pdf', filename = "Output_figures/olm_network_sig_only", height = 15, width = 20, labels = colnames(matrix.r.half), label.cex = 2)
```




#### Sqrt of r-squared of regression summary

Now I want to check to see if the OLS network diagram is any different if we use the square root of the coefficient of determination from the linear odels instead of simply the cor function.

```{r}
olm.reg.r2 <- lapply(olm.traits.list, function(model.list){
  unlist(lapply(model.list, r_squared))
})

olm.reg.r2 <- as.data.frame(olm.reg.r2)
colnames(olm.reg.r2) <- colnames(table.matrix)
row.names(olm.reg.r2) <- colnames(olm.reg.r2)
olm.reg.r2 <- t(olm.reg.r2) #now the right way around
diag(olm.reg.r2) <- 0 #better for network diagram , though having 1 in the diagonals is better for ellipse table


olm.coeff <- lapply(olm.traits.list, function(model.list){
  unlist(lapply(model.list, extract_slope))
})

olm.coeff <- as.data.frame(olm.coeff)
colnames(olm.coeff) <- colnames(table.matrix)
row.names(olm.coeff) <- colnames(olm.coeff)
olm.coeff <- t(olm.coeff) #now the right way around
olm.coeff.abs <- abs(olm.coeff)

olm.neg.slopes <- olm.coeff < 0
olm.reg.r <- sqrt(olm.reg.r2)
olm.reg.r[olm.neg.slopes] <- -1*olm.reg.r[olm.neg.slopes]

#olm.coeff.half < 0 == olm.coeff < 0 #manual check of what this line is meant to give you (if each logical matrix was its own object allowing the code to work i presume) gives them being equal hence slightly different coefficients never change sign so we don't have to worry about this for plotting purposes

#now to set all r-values for non-significant relationships to 0
olm.reg.r.sig.only <- olm.reg.r
olm.reg.r.sig.only[!sig.models.olm] <- 0

qgraph(table.matrix, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
qgraph(olm.reg.r, layout = 'spring', label.cex = 0.9, label.scale = FALSE)

qgraph(sig.only.r.olm, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
qgraph(olm.reg.r.sig.only, layout = 'spring', label.cex = 0.9, label.scale = FALSE)

qgraph(olm.reg.r.sig.only, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
qgraph(sig.only.r.half, layout = 'spring', label.cex = 0.9, label.scale = FALSE)

```


```{r}
olm.reg.r.sig.only - sig.only.r.olm
sort(abs(olm.reg.r.sig.only - sig.only.r.olm))

mean(abs(olm.reg.r.sig.only - sig.only.r.olm))
median(abs(olm.reg.r.sig.only - sig.only.r.olm))
```


### Only r > 0.2

```{r}
#going to try the network to see how much it gets altered if we change the definition of 'significant' to p< 0.05 AND r > 0.2

olm.reg.r.sig.only2 <- olm.reg.r.sig.only
olm.reg.r.sig.only2[abs(olm.reg.r.sig.only2) < 0.2] <- 0
olm.reg.r.sig.only2 == olm.reg.r.sig.only # RBT-LM, RBT-SM, LA-FM, and LM-SM

qgraph(olm.reg.r.sig.only2, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
qgraph(olm.reg.r.sig.only, layout = 'spring', label.cex = 0.9, label.scale = FALSE)

qgraph(olm.reg.r.sig.only2, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
qgraph(sig.only.r.half2, layout = 'spring', label.cex = 0.9, label.scale = FALSE)

qgraph(olm.reg.r.sig.only, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
qgraph(sig.only.r.half, layout = 'spring', label.cex = 0.9, label.scale = FALSE)

qgraph(table.matrix, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
qgraph(matrix.r.half, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
```

#### Only r > 0.3

```{r}
max(abs(olm.reg.r.sig.only2)) 
max(abs(sig.only.r.half2))
abs(olm.reg.r.sig.only2) > 0.9 #only FM-FWW are above 90 and, since it would not also get rid of other relstionships that we onsider to be trivial, hence we will not impose this upper limit
abs(sig.only.r.half2) > 0.9 # only FM-FWW again would be excluded by this
abs(olm.reg.r.sig.only2) < 0.3 #many correlations disappear

abs(olm.reg.r.sig.only2) > 0.8 # this excludes both FM-FWW and LA-LM
abs(sig.only.r.half2) > 0.8 # same with the pgls 

# now lets try 0.2 < r < 0.9
olm.reg.r.sig.only3 <- olm.reg.r.sig.only2
olm.reg.r.sig.only3[abs(olm.reg.r.sig.only3) < 0.2 | abs(olm.reg.r.sig.only3) > 0.9] <- 0 # additionally removes FM-FWW
qgraph(olm.reg.r.sig.only3, layout = 'spring', label.cex = 0.9, label.scale = FALSE) # network behaves surprisingly similar to original
sig.only.r.half3 <- sig.only.r.half2
sig.only.r.half3[abs(sig.only.r.half3) < 0.2 | abs(sig.only.r.half3) > 0.9] <- 0
qgraph(olm.reg.r.sig.only3, layout = 'spring', label.cex = 0.9, label.scale = FALSE) 
qgraph(sig.only.r.half3, layout = 'spring', label.cex = 0.9, label.scale = FALSE)

# now lets try r > 0.3
olm.reg.r.sig.only4 <- olm.reg.r.sig.only2
olm.reg.r.sig.only4[abs(olm.reg.r.sig.only4) < 0.3 & abs(olm.reg.r.sig.only4) > 0] <- 0 #removes SLA-SM, RH-LM, MH-LM, MH-FWW, and RBT-RH along with FM-FWW
qgraph(olm.reg.r.sig.only4, layout = 'spring', label.cex = 0.9, label.scale = FALSE) 
sig.only.r.half4 <- sig.only.r.half2
sig.only.r.half4[abs(sig.only.r.half4) < 0.3 & abs(sig.only.r.half4) > 0] <- 0 #removes RBT-FWW, RBT-FM, RBT-LA, SD-RB, RH-(SLA, LA, LM)
qgraph(sig.only.r.half4, layout = 'spring', label.cex = 0.9, label.scale = FALSE) 
qgraph(olm.reg.r.sig.only4, layout = 'spring', label.cex = 0.9, label.scale = FALSE) 


# now lets try 0.3 > r > 0.8
olm.reg.r.sig.only5 <- olm.reg.r.sig.only4
olm.reg.r.sig.only5[abs(olm.reg.r.sig.only5) > 0.8] < 0 #additionally removes LA-LM
qgraph(olm.reg.r.sig.only5, layout = 'spring', label.cex = 0.9, label.scale = FALSE) # not very informative as it makes it harder to see the distinctness of groups

sort(olm.reg.r.sig.only2)

olm.reg.r.sig.only4^2

median(abs(olm.reg.r.sig.only4[abs(olm.reg.r.sig.only4) > 0]))
mean(abs(olm.reg.r.sig.only4[abs(olm.reg.r.sig.only4) > 0]))

#using more colorblind-friendly colors:
qgraph(olm.reg.r.sig.only4, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'colorblind')
qgraph(sig.only.r.half4, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'colorblind') 

max(abs(sig.only.r.half4[sig.only.r.half4 > 0]))
```
When we apply progressively more stringent requirement on relationship strength in order for it to contribute to the network, SLA emerges as the most central trait as many of the relationships between MH and others and RBT and others were relatively weak (r-squared/pseudo-r-squared < 0.9) and the peripheral nature of RH and SD is highlighted. However, general relationships were found to be consistent




### Different arguments
```{r}
qgraph(olm.reg.r.sig.only, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'classic')
qgraph(olm.reg.r.sig.only, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'colorblind')
qgraph(olm.reg.r.sig.only, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'gray')
qgraph(olm.reg.r.sig.only, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'Hollywood')
qgraph(olm.reg.r.sig.only, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'Borkulo')
qgraph(olm.reg.r.sig.only, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'gimme')
qgraph(olm.reg.r.sig.only, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'TeamFortress')
qgraph(olm.reg.r.sig.only, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'Reddit')
qgraph(olm.reg.r.sig.only, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'Leuven')
qgraph(olm.reg.r.sig.only, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'Fried')


#using more colorblind-friendly colors:
qgraph(olm.reg.r.sig.only4, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'colorblind')
qgraph(sig.only.r.half4, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'colorblind') 

pdf("colorblind_pgls_network_0.3.pdf")
qgraph(sig.only.r.half4, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'colorblind') 
dev.off()

pdf("colorblind_OLS_network_0.3.pdf")
qgraph(olm.reg.r.sig.only4, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'colorblind') 
dev.off()
```

### Using minimum values instead

```{r}
qgraph(olm.reg.r, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'colorblind')
qgraph(olm.reg.r, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'colorblind', minimum = 0.3)
qgraph(olm.reg.r.sig.only4, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'colorblind')

qgraph(olm.reg.r, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'colorblind', minimum = 0.3, posCol = list('blue', 'blue'), negCol = list('red', 'red'), edge.width = 1.5)

qgraph(matrix.r.half, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'colorblind')
qgraph(matrix.r.half, layout = 'spring', label.cex = 0.9, label.scale = FALSE, theme = 'colorblind', minimum = 0.3)
```


## Looking at Change in Relationships Upon Accounting for Phylogeny

```{r fig.height = 8, fig.width = 10}
#to run model of lambda values against change in r-squared (or r). Lets try r first
  #first to calculate the change in r by subtracting the matrices (now that i've gone back to change the order so FWW is before FW on the original dataset from which the OLM correlation is calculated)
delta.r <-  matrix.r - table.matrix #need the pgls matrix minus the OLM one
delta.r <- as.data.frame(delta.r)

pgls.r2 <- matrix.r
pgls.r2 <- matrix.r^2

olm.r2 <- table.matrix
olm.r2 <- olm.r2^2
delta.r2 <- as.data.frame(pgls.r2 - olm.r2)


#to plot and have a look at these
corrplot(as.matrix(delta.r2), method = 'color') #makes more sense like this as shaded cells than ellipses
mtext(text = "Dependent Variable", side = 2, line = 1)
mtext(text = "Independent Variable", side = 3, line = 1.25)

corrplot(as.matrix(delta.r), method = 'color')

# #now to export this graph
# pdf('delta_r2_colour_table.pdf')
# corrplot(as.matrix(delta.r2), method = 'color')
# mtext(text = "Dependent Variable", side = 2, line = 1)
# mtext(text = "Independent Variable", side = 3, line = 1.25)
# dev.off()
```
The corrplot way has intuitive and consistent colors but is very light bc the colour scale is set to max at much larger values.
The heat colours matrix doesn't have as intuitive a colour matrix


### Testing the correlations with lambda value

Now to actually formally test a correlation between lambda magnitude and change in correlation coefficient
```{r fig.height = 8, fig.width = 10}
# lambda.values <- as.data.frame(lambda.matrix)
# 
# lambda.vs.delr <- data.frame('lambda' = unlist(lambda.values), 'delta_r' = unlist(delta.r))
# lambda.vs.delr <- lambda.vs.delr[!is.na(lambda.vs.delr$lambda),]
# ggplot(lambda.vs.delr, aes(x = lambda, y = delta_r)) + geom_point()
# 
# lambda.vs.abs.delr <- lambda.vs.delr
# lambda.vs.abs.delr$delta_r <- abs(lambda.vs.abs.delr$delta_r)
# ggplot(lambda.vs.abs.delr, aes(x = lambda, y = delta_r)) + geom_point() + geom_abline(slope = 0.136, intercept = 0.046)
# ggplot(lambda.vs.delr, aes(x = lambda, y = delta_r)) + geom_point()
# 
# summary(lm(delta_r ~ lambda, data = lambda.vs.delr)) #no relationship
# summary(lm(delta_r ~ lambda, data = lambda.vs.abs.delr)) #is a positive relationship but the main cloud of the data looks negatively correlated hence I suspect this is only because of the lambda = 0.000 points skewing things

# length(lambda.vs.abs.delr$delta_r)
# length(lambda.vs.abs.delr$lambda)
# 
# lambda.vs.abs <- lambda.vs.abs.delr[!lambda.vs.abs.delr$lambda < 0.001, ]
# sum(lambda.vs.abs.delr$lambda < 0.001)
# 
# summary(lm(delta_r ~ lambda, data = lambda.vs.abs)) #no relationship now that those lambda = 0.000 were removed, however I'm not sure 
```


```{r}
#to retest with r-squared
pgls.r2 <- matrix.r
pgls.r2 <- matrix.r^2

olm.r2 <- table.matrix
olm.r2 <- olm.r2^2
delta.r2 <- as.data.frame(pgls.r2 - olm.r2)


lambda.vs.delr2 <- data.frame('lambda' = unlist(as.data.frame(lambda.matrix)), 'delta_r2' = unlist(delta.r2))
lambda.vs.delr2 <- lambda.vs.delr2[!is.na(lambda.vs.delr2$lambda),]
ggplot(lambda.vs.delr2, aes(x = lambda, y = delta_r2)) + geom_point()
summary(lm(delta_r2 ~ lambda, data = lambda.vs.delr2)) #nope no relationship

#now try with absolute values instead
lambda.vs.delr2.abs <- lambda.vs.delr2
lambda.vs.delr2.abs$delta_r2 <- abs(lambda.vs.delr2.abs$delta_r2)
ggplot(lambda.vs.delr2.abs, aes(x = lambda, y = delta_r2)) + geom_point() + geom_abline(slope = 0.0524, intercept = 0.03728) #looks less like a smooth flow so I thing keeping it the other way would be best to avoid trucation at zero
summary(lm(delta_r2 ~ lambda, data = lambda.vs.delr2.abs)) #borderline actually but mostly due to

lambda.vs.delr2.2 <- lambda.vs.delr2[!lambda.vs.delr2$lambda < 0.01,]
lambda.vs.delr2.abs.2 <- lambda.vs.delr2.abs[!lambda.vs.delr2.abs$lambda < 0.01,]
ggplot(lambda.vs.delr2.2, aes(x = lambda, y = delta_r2)) + geom_point()
summary(lm(delta_r2 ~ lambda, data = lambda.vs.delr2.2)) #significant and positive more ike the cloud, ~11% of variatin accounted for
summary(lm(delta_r2 ~ lambda, data = lambda.vs.delr2.abs.2)) #significant and negative ~5% of variation accounted for

# x^2 #does element by element
# x%*%x #matrix multiplication

ggplot(lambda.vs.delr2, aes(x = lambda, y = delta_r2)) + geom_point() + geom_abline(slope = 0.27870, intercept = -0.22279)
ggplot(lambda.vs.delr2.abs, aes(x = lambda, y = delta_r2)) + geom_point() + geom_abline(slope = -0.13919, intercept = 0.16934)

pdf('lambda_vs_delta_r2_and_abs_delta_r2_scatter.pdf')
ggplot(lambda.vs.delr2, aes(x = lambda, y = delta_r2)) + geom_point() + geom_abline(slope = 0.27870, intercept = -0.22279)
ggplot(lambda.vs.delr2.abs, aes(x = lambda, y = delta_r2)) + geom_point() + geom_abline(slope = -0.13919, intercept = 0.16934)
dev.off()



```

So there is a relatively weak but present relationship between lambda magnitude and change in r-squared but not so much between lambda and r itself, once the lambda = 0.000 outliers were excluded since the estimation of lambda for those seemed to be a bit volatile. When the absolute magnitude of delta r2 is used the relationship becomes negative compared to positive when the sign is kept, whch somewhat suggests that lower lambda values tend to more often decrease the strength of the correlation, where as higher lambda values tend to increase it but I think this is almost entirely explained by the fact that the highest-lambda correlations tended to also be the reproductive traits, which we've already seen to be amplified by adding pgls, whereas the converse is true for the stem-habit traits.

Perhaps colour by dependent variable and see if this is true at all and if the pattern is observable.


```{r}
#to colour by dependent variable we need to first create another column in our plotting dataframe that specifies the dep. var. Since the row names of the matrices unlisted are the same those row names are also those of the combined dataframe

#The acronym of the trait is the independent variable (column of dataframe) but the following number corresponds to the dependent variable (row number)

# row.names(lambda.vs.delr2)[grepl("1$", row.names(lambda.vs.delr2))]
# lambda.vs.delr2.col <- cbind(lambda.vs.delr2, rep(NA, length(lambda.vs.delr2$delta_r2)))
# names(lambda.vs.delr2.col) <- c("lambda", "delta_r2", "dep_var")
# 
# lambda.vs.delr2.col$dep_var[grepl("1$", row.names(lambda.vs.delr2))] <- 'MH'
# lambda.vs.delr2.col$dep_var[grepl("2$", row.names(lambda.vs.delr2))] <- 'RH'
# lambda.vs.delr2.col$dep_var[grepl("3$", row.names(lambda.vs.delr2))] <- 'SD'
# lambda.vs.delr2.col$dep_var[grepl("4$", row.names(lambda.vs.delr2))] <- 'RBT'
# lambda.vs.delr2.col$dep_var[grepl("5$", row.names(lambda.vs.delr2))] <- 'SLA'
# lambda.vs.delr2.col$dep_var[grepl("6$", row.names(lambda.vs.delr2))] <- 'LA'
# lambda.vs.delr2.col$dep_var[grepl("7$", row.names(lambda.vs.delr2))] <- 'LM'
# lambda.vs.delr2.col$dep_var[grepl("8$", row.names(lambda.vs.delr2))] <- 'FWW'
# lambda.vs.delr2.col$dep_var[grepl("9$", row.names(lambda.vs.delr2))] <- 'FM'
# lambda.vs.delr2.col$dep_var[grepl("10$", row.names(lambda.vs.delr2))] <- 'SM'
# 
# 
# ggplot(lambda.vs.delr2.col, aes(x = lambda, y = delta_r2, color = dep_var)) + geom_point() + scale_color_manual(values = c('MH' = 'red', 'RH' = 'darkorange2', 'SD' = 'darkgoldenrod3', 'RBT' = 'chocolate4', 'SLA' = 'chartreuse', 'LA' = 'darkolivegreen3', 'LM' = 'chartreuse4', 'FWW' = 'darkslateblue', 'FM' = 'darkviolet', 'SM' = 'darkorchid4'))
# 
# ggplot(lambda.vs.delr2.col, aes(x = lambda, y = delta_r2, color = dep_var)) + geom_point() + scale_color_manual(values = c('MH' = 'red', 'RH' = 'red', 'SD' = 'orange', 'RBT' = 'orange', 'SLA' = 'yellow', 'LA' = 'yellow', 'LM' = 'blue', 'FWW' = 'darkslateblue', 'FM' = 'darkviolet', 'SM' = 'darkorchid4'))
# 
# ggplot(lambda.vs.delr2.col, aes(x = lambda, y = delta_r2, color = dep_var)) + geom_point() + scale_color_manual(values = c('MH' = 'red', 'RH' = 'red', 'SD' = 'red', 'RBT' = 'red', 'SLA' = 'yellow', 'LA' = 'yellow', 'LM' = 'blue', 'FWW' = 'violet', 'FM' = 'violet', 'SM' = 'violet'))
```


Now to redo the stats analysis between lambda and delta r2 but this time with half as many point as I'm only considering the models with the higher lambda so there will initially be the same number of points but half will be exact duplicates and so relatively easy to remove

```{r fig.height = 8, fig.width = 10}
#first need to remale the delta.r2 table in ider to have something to unlist
  # matrix.r2.half from pgls already exists, since i used it to get to the r value initially
  #olm.r2 already exists from the first time i ran this analysis since it shouldn't have changed
delta.r2.half <- as.data.frame(matrix.r2.half - olm.r2)

corrplot(as.matrix(delta.r2.half), method = 'color') #apparently more colour
corrplot(as.matrix(delta.r2), method = 'color', ylab = 'Dependent Variable', xlab = 'Independent Variable') #

plot(lambda.matrix, col = heat.colors(30, rev = TRUE), breaks = 30, na.col= "black", key = list(side = 1, cex.axis = 1.00), fmt.key="%.3f", xlab = "", ylab = "", main = "", axis.col = list(side = 3), asp = TRUE)
corrplot(as.matrix(delta.r2), method = 'color')

plot(lambda.half, col = heat.colors(10, rev = TRUE), breaks = c(0.400, 0.450, 0.500, 0.550, 0.600, 0.650, 0.700, 0.750, 0.800, 0.850, 0.900), na.col= "black", key = list(side = 1, cex.axis = 1.00), fmt.key="%.3f", xlab = "", ylab = "", main = "", axis.col = list(side = 3), asp = TRUE)
corrplot(as.matrix(delta.r2.half), method = 'color')

# pdf('lambda_table_symmetrical.pdf')
# plot(lambda.half, col = heat.colors(10, rev = TRUE), breaks = c(0.000, 0.450, 0.500, 0.550, 0.600, 0.650, 0.700, 0.750, 0.800, 0.850, 0.900), na.col= "black", key = list(side = 1, cex.axis = 1.00), fmt.key="%.3f", xlab = "", ylab = "", main = "", axis.col = list(side = 3))
# mtext(text = "Independent Variable", side = 3, line = 2.5)
# mtext(text = "Dependent Variable", side = 2, line = 2.5)
# dev.off()
# 
# pdf('delta_r2_table_symmetrical.pdf')
# corrplot(as.matrix(delta.r2.half), method = 'color')
# mtext(text = "Dependent Variable", side = 2, line = 1)
# mtext(text = "Independent Variable", side = 3, line = 1.25)
# dev.off()


# lambda.vs.delr2.half <- data.frame('lambda' = unlist(as.data.frame(lambda.half)), 'delta_r2' = unlist(delta.r2.half))
# lambda.vs.delr2.half <- lambda.vs.delr2.half[!is.na(lambda.vs.delr2.half$lambda),]
# lambda.vs.delr2.half <- cbind(lambda.vs.delr2.half, lambda.vs.delr2.col$dep_var)
# colnames(lambda.vs.delr2.half) <- c('lambda', 'delta_r2', 'dep_var')
# lambda.vs.delr2.half <- lambda.vs.delr2.half[!duplicated(lambda.vs.delr2.half$delta_r2),] #there are exactly 45 duplicates now and thy make the same shape on the plot so we can be sure this is done correctly 
# ggplot(lambda.vs.delr2.half, aes(x = lambda, y = delta_r2)) + geom_point() +geom_abline(intercept = -0.15094, slope = 0.15872)
# summary(lm(delta_r2 ~ lambda, data = lambda.vs.delr2.half)) #yeah I suspect that single interfering lambda = 0.00010 point is causing trouble so I'll have to exclude it and run it again
# lambda.vs.delr2.half2 <- lambda.vs.delr2.half[lambda.vs.delr2.half$lambda > 0.01,]
# ggplot(lambda.vs.delr2.half, aes(x = lambda, y = delta_r2)) + geom_point() + geom_abline(intercept = -0.4324, slope = 0.5407)
# summary(lm(delta_r2 ~ lambda, data = lambda.vs.delr2.half2)) # significant positive relationship accounting for ~ 20% of the variation so even more of a relationship now that 
# ggplot(lambda.vs.delr2.half2, aes(x = lambda, y = delta_r2)) + geom_point() + geom_abline(intercept = -0.4324, slope = 0.5407)
# 
# lambda.vs.delr2.half.abs <- lambda.vs.delr2.half
# lambda.vs.delr2.half.abs$delta_r2 <- abs(lambda.vs.delr2.half$delta_r2)
# lambda.vs.delr2.half.abs2 <- lambda.vs.delr2.half.abs[lambda.vs.delr2.half.abs$lambda > 0.01,]
# summary(lm(delta_r2 ~ lambda, data = lambda.vs.delr2.half.abs2))
# 
# ggplot(lambda.vs.delr2.half.abs, aes(x = lambda, y = delta_r2)) + geom_point() + geom_abline(slope = -0.24265, intercept = 0.25193)

#now to export these
# pdf('lambda_vs_delta_r2_and_abs_delta_r2_scatter_using_higher_lambda_model.pdf')
# ggplot(lambda.vs.delr2.half2, aes(x = lambda, y = delta_r2)) + geom_point() + geom_abline(intercept = -0.4324, slope = 0.5407)
# ggplot(lambda.vs.delr2.half.abs, aes(x = lambda, y = delta_r2)) + geom_point() + geom_abline(slope = -0.24265, intercept = 0.25193) + ggtitle('this line not quite significant, P = 0.0695')
# dev.off()
```



Now try and make the network diagram with only the lines for statistically significant relationships

```{r fig.height = 8, fig.width = 10}
#now to do the same for the non-pgls analysis, though I don't expect it to change much
sig.models.olm <- data.frame('MH' = c(0,0,0,0,0,0,0,0,0,0), 'RH' = c(1,0,0,0,0,0,0,0,0,0), 'SD' = c(1,1,0,0,0,0,0,0,0,0), 'RBT' = c(1,1,1,0,0,0,0,0,0,0), 'SLA' = c(1,1,1,1,0,0,0,0,0,0), 'LA' = c(1,1,1,1,1,0,0,0,0,0), 'LM' = c(1,1,0,1,1,1,0,0,0,0), 'FWW' = c(1,0,0,1,1,0,1,0,0,0), 'FM' = c(0,0,0,1,1,1,1,1,0,0), 'SM' = c(0,0,0,1,1,0,1,1,1,0)) #will need to add this upper-triangle to its transpose to get the full one
dim(sig.models.olm) #all good
as.matrix(sig.models.olm)
  t(sig.models.olm)
sig.models.olm <- sig.models.olm + t(sig.models.olm)
sig.models.olm <- sig.models.olm == 1
sig.only.r.olm <- table.matrix
sig.only.r.olm[!sig.models.olm] <- 0

qgraph(sig.only.r.olm, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
qgraph(table.matrix, layout = 'spring', label.cex = 0.9, label.scale = FALSE)

network.diag.olm.sig <- qgraph(sig.only.r.olm, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
network.diag.pgls.sig <- qgraph(sig.only.r.half, layout = 'spring', label.cex = 0.9, label.scale = FALSE)

qgraph(sig.only.r.olm, layout = 'spring', label.cex = 0.9, label.scale = FALSE)
qgraph(sig.only.r.half, layout = 'spring', label.cex = 0.9, label.scale = FALSE)

#qgraph(network.diag.pgls.sig, filetype = 'pdf', filename = "pgls_network_sig_only", height = 15, width = 20, labels = colnames(matrix.r.half), label.cex = 2)
#qgraph(network.diag.olm.sig, filetype = 'pdf', filename = "olm_network_sig_only", height = 15, width = 20, labels = colnames(matrix.r.half), label.cex = 2)

```

Now trying to see if there's a way of manually adjusting the position of some og the nodes so that SLA is not in the way of two different lines passing through it

```{r, fig.height = 8, fig.width = 10}
original.node.layout <- network.diag.pgls.sig$layout
network.diag.pgls.sig2 <- network.diag.pgls.sig
plot(network.diag.pgls.sig)
network.diag.pgls.sig$layout
network.diag.pgls.sig2$layout
plot(network.diag.pgls.sig2) #good you can plot the stored object after you alter it
network.diag.pgls.sig2$layout[5,2] <- 0.0500000 #yay this actually works to manually move the position of the SLA node!!
network.diag.pgls.sig2$layout[5,1] <- network.diag.pgls.sig$layout[5,1]

qgraph(network.diag.pgls.sig2, filetype = 'pdf', filename = "pgls_network_sig_only2", height = 15, width = 20, labels = colnames(matrix.r.half), label.cex = 2)
```


### Using Slope Coefficient

```{r}
#need to redo the change in r^2 as change in effect strength since r2 is not really comparable between these models
coeff <- t(matrix.table.c)
colnames(coeff) <- row.names(coeff)
coeff.abs <- abs(coeff)


# install.packages("nlme")
# summary(lm(max_height_m ~ relative_height_by_girth, data = eucs.scaled.data))
# summary(gls(max_height_m ~ relative_height_by_girth, data = eucs.scaled.data, na.action = na.omit)) #no r-squared value at all so would need to actively and separately calculate it

#now to extract the coefficients of the lm models and take the absolute value in order to subtract these values from the pgls coefficients magnitude


delta.c <- as.matrix(coeff.abs - olm.coeff.abs)
corrplot(delta.c, method = "color")

plot(as.matrix(coeff)) #this matrix (pgls coefficients) is not symmetrical so maybe if we do the transformation that takes the higher-lambda halves for comparison as well

pgls.coeff.half <- matrix(NA, 10, 10)
pgls.coeff.half[log2] <- as.matrix(coeff)[log2]
pgls.coeff.half[!log2] <- t(as.matrix(coeff))[!log2]
rownames(pgls.coeff.half) <- rownames(coeff)
colnames(pgls.coeff.half) <- rownames(coeff)

olm.coeff.half <- matrix(NA, 10, 10)
olm.coeff.half[log2] <- as.matrix(olm.coeff)[log2]
olm.coeff.half[!log2] <- t(as.matrix(olm.coeff))[!log2]
rownames(olm.coeff.half) <- rownames(olm.coeff)
colnames(olm.coeff.half) <- rownames(olm.coeff)

delta.c.half <- abs(pgls.coeff.half) - abs(olm.coeff.half)
corrplot(delta.c.half, method = "color")
corrplot(as.matrix(delta.r2.half), method='color')
corrplot(delta.c, method = "color")

#corrplot(pgls.coeff.half-olm.coeff.half, method = 'color')

corrplot(delta.c.half/(6*max(delta.c.half)), method = "color")
corrplot(as.matrix(delta.r2.half), method='color')

# pdf('change_in_slope_magnitude_colscaled.pdf')
# corrplot(delta.c.half/(6*max(delta.c.half)), method = "color")
# dev.off()
# 
# pdf('change_in_slope_magnitude.pdf')
# corrplot(delta.c.half, method = "color")
# dev.off()

max(abs(delta.c))
min(abs(delta.c))


```

```{r}
lambda.vs.delcoeff <- data.frame('lambda' = unlist(as.data.frame(lambda.matrix)), 'delta_coeff' = unlist(as.data.frame(delta.c)))
lambda.vs.delcoeff <- lambda.vs.delcoeff[!is.na(lambda.vs.delcoeff$lambda),]
ggplot(lambda.vs.delcoeff, aes(x = lambda, y = delta_coeff)) + geom_point()
summary(lm(delta_r2 ~ lambda, data = lambda.vs.delr2)) #nope no relationship

lambda.vs.delcoeff.2 <- lambda.vs.delcoeff[!lambda.vs.delcoeff$lambda < 0.01,]
lambda.vs.delcoeff.abs.2 <- lambda.vs.delcoeff.abs[!lambda.vs.delcoeff.abs$lambda < 0.01,]
ggplot(lambda.vs.delcoeff.2, aes(x = lambda, y = delta_coeff)) + geom_point()
summary(lm(delta_coeff ~ lambda, data = lambda.vs.delcoeff.2)) #
summary(lm(delta_coeff ~ lambda, data = lambda.vs.delcoeff.abs.2)) #

lambda.vs.delcoeff.abs <- lambda.vs.delcoeff.2
lambda.vs.delcoeff.abs$delta_coeff <- abs(lambda.vs.delcoeff.abs$delta_coeff)
ggplot(lambda.vs.delcoeff.abs, aes(x = lambda, y = delta_coeff)) + geom_point() + geom_abline(slope = 0.0524, intercept = 0.03728) #looks less like a smooth flow so I thing keeping it the other way would be best to avoid trucation at zero
summary(lm(delta_coeff ~ lambda, data = lambda.vs.delcoeff.abs)) #borderline actually but mostly due to

```


```{r}
change.in.sig <- sig.models == sig.models.olm

sig.models.olm == t(sig.models.olm) #all the same so it's just the pgls version that's the problem

sig.models.half <- matrix(NA, 10, 10)
sig.models.half[log2] <- sig.models[log2]
sig.models.half[log3] <- t(sig.models)[log3]
sig.models.half[is.na(sig.models.half) == TRUE] <-FALSE
rownames(sig.models.half) <- rownames(sig.models)
colnames(sig.models.half) <- rownames(sig.models.half)

sig.models.half == t(sig.models.half) #now good

change.in.sig2 <- sig.models.half == sig.models.olm


change.in.sig #definitely different
change.in.sig2 #this is only relating to what's on the network diagrams so only half the pgls models and only choosing the higher lambda of those where it is sig one way and not the other

```
Loss of significance upon accounting for phylogeny:
MH-SD
MH-LM
MH-FWW
RH-RBT
RBT-LM
RBT-SM
SLA-LA


Gain of significance upon accounting for phylogeny:
LA-FWW
LA-SM

```{r}
corrplot(delta.c.half, method = "color", p.mat = !change.in.sig2, pch = 2)

pdf("Output_figures/delta_c_sig.pdf")
corrplot(delta.c.half, method = "color", p.mat = !change.in.sig2, pch = 2)
dev.off()
max(delta.c.half) #~ 0.2
min(delta.c.half) #~ -0.3

```


# Checking if reciprcal differences in slope outside the confidence interval

Is the difference between reciprocal-model slope coefficients always within the confidence intervals?

```{r}
diff.c.abs <- abs(olm.coeff - t(olm.coeff))

sort(diff.c.abs) #there's a .1, .09, and .8 then the rets are less than .05

#need to write an extract se function now to determine if these differences are within the standard errors

# v <- summary(olm.rh.std)
# v$call$formula[[3]]
# str(v)
# olm.rh.std$call$formula[[3]]
# v$coefficients[2,2]
# olm.rh.std$coefficients[c(as.character(olm.rh.std$call$formula[[3]])),c('Std. Error')]
# v2 <- summary(max.height.rh)
# str(v2)
# v2$coefficients[c('max.height.rh'),c('Std. Error')]
# row.names(v2$coefficients)
# v2$call$formula[[3]]


extract_SE <- function(model) { 
  #this extract the standard error of the slope coefficient for a single independent variable (or maybe the first one mentioned if there are more than one but it is untested)
  x <- summary(model)
  s.e.val <- x$coefficients[c(as.character(x$call$formula[[3]])),c('Std. Error')]
  return(s.e.val)
}

extract_SE(olm.rh.std)

extract_P <- function(model) {
  #extracts the P-value for the slope coefficient of the independent variable in a bvariate correlation
  x <- summary(model)
  P <- x$coefficients[c(as.character(x$call$formula[[3]])), c('Pr(>|t|)')]
  return(P)
}
extract_P(olm.rh.std) 

#now for extracting the SE value for each model

olm.coeff.SE <- lapply(olm.traits.list, function(model.list){
  unlist(lapply(model.list, extract_SE))
})

olm.coeff.SE <- as.data.frame(olm.coeff.SE)
colnames(olm.coeff.SE) <- colnames(table.matrix)
row.names(olm.coeff.SE) <- colnames(olm.coeff.SE)
olm.coeff.SE <- t(olm.coeff.SE) #now the right way around

sum(diff.c.abs > olm.coeff.SE) # 6 models (including reciprocals) where the difference between the slope coefficient of the model and that of its reciprocal exceeds the standard error of the former model
#the models are RH, SD nd RBT - LA and LA-RH, SD and RBT (they are reciprocals of each other)
#note the difference matrix is symmetrical, but the the SE-matrix is not

olm.r2

```


```{r}

olm.r2.reg <- lapply(olm.traits.list, function(model.list){
  unlist(lapply(model.list, r_squared))
})

olm.r2.reg <- as.data.frame(olm.r2.reg)
colnames(olm.r2.reg) <- colnames(table.matrix)
row.names(olm.r2.reg) <- colnames(olm.r2.reg)
olm.r2.reg <- t(olm.r2.reg) #now it's around the right way with the row indicating the dependent variable and colun the independent variable
olm.r2.reg




extract_n <- function(model) {
  x <- summary(model)
  n <- x$df[[2]]
  return(n)
}

# olm.mh.fms$qr$rank
# 
# v3 <- summary(olm.mh.fms)
# str(v3)
# v3$df
# extract_n(olm.mh.fms)

olm.n <- lapply(olm.traits.list, function(model.list){
  unlist(lapply(model.list, extract_n))
})

olm.n <- as.data.frame(olm.n)
colnames(olm.n) <- colnames(table.matrix)
row.names(olm.n) <- colnames(olm.n)
olm.n <- t(olm.n) #now it's around the right way with the row indicating the dependent variable and colun the independent variable
olm.n

```

